


# Basic Import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
# Modelling
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression, Ridge,Lasso
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import RandomizedSearchCV
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
import warnings





df=pd.read_csv('data/stud.csv')





df.head()





X=pd.DataFrame(df)
X=df.drop(columns=('math_score'),axis=1)
X.head()



for i ,col in enumerate(X.select_dtypes(include=['object']).columns):
    print(f"Categories in '{col}' variable:    {df[col].unique()}")
dim=[]
for i ,col in enumerate(X.select_dtypes(include=['object']).columns):
    dim.append(len(df[col].unique()))
print( " number of unique values in every category ")
dim=dim+[1,1]
dim


y=df['math_score']


y





# Create Column Transformer with 3 types of transformers
num_features = df['reading_score'].values.reshape(-1,1)
#reshape('rows,columns)

from sklearn.discriminant_analysis import StandardScaler

numeric_transformer = StandardScaler()
print(numeric_transformer.fit(num_features)) #it returns the fitted transformer itself.Fits the transformer to num_features.Prints the transformer with its learned parameters
print(numeric_transformer.mean_)
print(numeric_transformer.scale_)
#standscaler( x-u)/s

print(numeric_transformer.transform(num_features))
print(numeric_transformer.inverse_transform(np.array([[0.19399858], [1.42747598]])))








from sklearn.preprocessing import OneHotEncoder

num_features = df.select_dtypes(include="object").columns
enc = OneHotEncoder(handle_unknown='ignore')
enc.fit(df[num_features])
enc.categories_
p=enc.transform(df[num_features]).toarray()

print(enc.transform(df[num_features]).toarray().tolist())
print(enc.inverse_transform([[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]]))
print(enc.inverse_transform([[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]]))
enc.get_feature_names_out(['gender','race_ethnicity','parental_level_of_education','lunch','test_preparation_course'])





from sklearn.compose import ColumnTransformer
import csv


num_features=X.select_dtypes(exclude='object').columns
cat_features=X.select_dtypes(include='object').columns

numeric_transformer=StandardScaler()
oh_transformer=OneHotEncoder()
preprocessor=ColumnTransformer(
    [
        ("OneHotEncoder",oh_transformer,cat_features),
        ("StandardScaler",numeric_transformer,num_features)
    ]
)
X_transformed = numeric_transformer.fit_transform(df[num_features])
print('StandardScaler')
print(numeric_transformer.mean_)
print(numeric_transformer.scale_)


oh_transformer.fit(df[cat_features])
oh_transformer.categories_
p=oh_transformer.transform(df[cat_features]).toarray()
print('oh_transformer')
print(oh_transformer.transform(df[cat_features]).toarray())
print(oh_transformer.inverse_transform(oh_transformer.transform(df[cat_features]).toarray()))

X_transformed = preprocessor.fit_transform(X)
oh_columns = oh_transformer.get_feature_names_out(cat_features)

csv_data = []

print("Transformation Table:")
print("-" * 50)

header = []

for col in X.select_dtypes(include=['object']).columns:
    header.append(f"{col}, {df[col].unique()}")  


header.extend(num_features)

csv_data.append(header)


print(" | ".join(header))

for i, row in enumerate(X_transformed):
    oh_arrays = []
    for cat_feature in cat_features:
        cat_indices = [i for i, col in enumerate(oh_columns) if col.startswith(cat_feature)]
        oh_array = row[cat_indices]
        oh_arrays.append(str(oh_array))
    
    numerical_values = [str(row[len(oh_columns) + j]) for j, _ in enumerate(num_features)]
    
    csv_data.append(oh_arrays + numerical_values)

    print(" | ".join(oh_arrays + numerical_values))

print("-" * 50)
output_file = "transformed_data.csv"
with open(output_file, mode="w", newline="") as file:
    writer = csv.writer(file)
    writer.writerows(csv_data)

print(f"Transformed data saved to {output_file}")





print(X_transformed.shape)#19  is the number of all elements in teh arrays 
print(X.shape)#number of columns



X=preprocessor.fit_transform(X)
X.shape





X1=pd.DataFrame(X)
X1.head(5).values.tolist()






# separate dataset into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
print(X_train.shape, X_test.shape)
print(y_train.shape,y_test.shape)











def evaluate_model(y_true, predicted):
    mae = mean_absolute_error(y_true, predicted)
    mse = mean_squared_error(y_true, predicted)
    rmse = np.sqrt(mean_squared_error(y_true, predicted))
    r2_square = r2_score(y_true, predicted)
    return mae, rmse, r2_square


models={
    "Linear Regression":LinearRegression(),
    "Lasso":Lasso(),
    "Ridge": Ridge(),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest Regressor": XGBRegressor(),
    "CatBoosting Regressor": CatBoostRegressor(verbose=False),
    "AdaBoost Regressor": AdaBoostRegressor()
}





















model_list = []
r2_list =[]
for i in range(len(list(models))):

    model = list(models.values())[i]    
    model.fit(X_train, y_train)

    y_train_pred=model.predict(X_train)
    y_test_pred=model.predict(X_test)
    
    model_train_mae , model_train_rmse , model_train_r2 = evaluate_model(y_train , y_train_pred)
    model_test_mae , model_test_rmse , model_test_r2 = evaluate_model(y_test, y_test_pred)

    print(list(models.keys())[i])
    model_list.append(list(models.keys())[i])
    print('Model performance for training')
    print('Root Mean Squared Error: {:.4f}'.format(model_train_rmse))
    print('Mean Abdolute Error: {:.4f}' .format(model_train_mae))
    print('R2 Score: {: .4f}'.format(model_train_r2))

    print('-'*35)

    print('Model performance for testing')
    print('Root Mean Squared Error: {:.4f}'.format(model_test_rmse))
    print('Mean Abdolute Error: {:.4f}' .format(model_test_mae))
    print('R2 Score: {: .4f}'.format(model_test_r2))

    r2_list.append(model_test_r2)
    
    print('='*35)
    print('\n')


y_test





pd.DataFrame(list(zip(model_list,r2_list)), columns=['Model Name', 'R2 Score']).sort_values(by=['R2 Score'],ascending=False)





lin_model=LinearRegression(fit_intercept=True)
lin_model=lin_model.fit(X_train, y_train)
y_pred=lin_model.predict(X_test)
score=r2_score(y_test,y_pred)*100
print('Acuraccy of the model is %.2f' %score)






f,ax=plt.subplots(1,1, figsize=(30,8))
sns.barplot(x=y_test,y=y_pred)
plt.show()



lin_model=DecisionTreeRegressor()
lin_model=lin_model.fit(X_train, y_train)
y_pred1=lin_model.predict(X_test)
score=r2_score(y_test,y_pred1)*100
print('Acuraccy of the model is %.2f' %score)


lin_model=Ridge()
lin_model=lin_model.fit(X_train, y_train)
y_pred2=lin_model.predict(X_test)
score=r2_score(y_test,y_pred2)*100
print('Acuraccy of the model is %.2f' %score)



pd.DataFrame(list([1,2,3,4,5])).plot()


f,ax=plt.subplots(2,len(list(models)), figsize=(50,20))
p=['skyblue','hotpink','yellow','lightgreen','violet','pink','#ff8000','#ff4d4d']
plt.suptitle("Training vs Testing",fontsize=35, fontweight='bold',color='blue')
for i in range(len(list(models))):

    model = list(models.values())[i]    
    model.fit(X_train, y_train)

    y_train_pred=model.predict(X_train)
    plt.subplot(2,len(list(models)),i+1)
    plt.xlim(0, 100)  
    plt.ylim(0, 100) 
    plt.xlabel(list(models.keys())[i]+" trained",fontweight = 30, fontsize = 20)
    plt.scatter(y_train, y_train_pred,color=p[i])
    plt.subplot(2,len(list(models)),i+1)



    model = list(models.values())[i]    
    model.fit(X_train, y_train)


    y_test_pred=model.predict(X_test)
    plt.subplot(2,len(list(models)),len(list(models))+i+1)
    plt.xlim(0, 100)  
    plt.ylim(0, 100) 
    plt.xlabel(list(models.keys())[i] +" tested",fontweight = 30, fontsize = 20)
    plt.scatter(y_test, y_test_pred,color=p[i])

plt.title('Training vs testing ')

plt.show()











class DebugLinearRegression(LinearRegression):
    def fit(self, X, y):
        print("Starting training...")
        print("Input data (X):", X)
        print("Target data (y):", y)
        super().fit(X, y)  # Call the original fit method
        print("Training complete. Coefficients:", self.coef_)
        print("Intercept:", self.intercept_)
        self.coefficient = self.coef_

model = DebugLinearRegression()
model.fit(X_train, y_train)
model.predict(X)
df=pd.read_csv('data/stud-Copy1.csv')
df['math_score_predicted_linear_regression']=model.predict(X)
df.to_csv('data/stud-Copy1.csv', index=False)
coefficients = model.coefficient
start = 0
ar = []
ar.extend(['','Coefficients','of','the','linear','regression',''])
arr=[]
for length in dim:
    end = start + length
    sublist = coefficients[start:end]
    arr.append("    &   ".join(map(str,sublist))) 
    start=end

print('-'*50)
print(ar)
with open(output_file, mode="a", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(ar)
    writer.writerow(arr)







lin_model = LinearRegression(fit_intercept=True)
lin_model = lin_model.fit(X_train, y_train)
y_pred = lin_model.predict([
   
 [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.19399858403803502, 0.3914918076496706],  
 [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.4274759816659497, 1.3132686840532009],  
 [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.7701085921181483, 1.6424747113401759],  
 [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, -0.8338992473185606, -1.5837443560721798],  
 [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6051577165806732, 0.45733301310706564]  

]) 
print('prediction des 5 premieres lignes vs les vraies valeures et la difference entre la prediction et la vrai valeure ')
print(y_pred )
print(y.head(5).tolist())
print(y_pred-y.head(5).tolist())
